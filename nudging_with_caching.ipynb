{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471d6fb8",
   "metadata": {},
   "source": [
    "# A simple implementation of nudging with caching\n",
    "- This code is mainly for testing the performance of nudging with caching, and is by no means a production-ready code.\n",
    "- The code compares the performance of nudging with caching against the baseline of using the base model alone with caching.\n",
    "- Currently only implemented for nudging within the same model family (e.g., Llama-2).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03178b5d",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0483bb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model: /extra/ucinlp1/llama-2/llama-2-70B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308f6c7e93f2477f9fa8b30fc9b387ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nudging model: /extra/ucinlp1/llama-2/llama-2-7B-chat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39f280ee84344569ff9971326262492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "NUDGING_MODEL_NAME = \"/extra/ucinlp1/llama-2/llama-2-7B-chat\"   # path to the nudging model\n",
    "BASE_MODEL_NAME = \"/extra/ucinlp1/llama-2/llama-2-70B\"          # path to the base model\n",
    "\n",
    "# NUDGING_MODEL_NAME = \"/extra/ucinlp1/Qwen/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "# BASE_MODEL_NAME = \"/extra/ucinlp1/Qwen/Qwen-2.5-Math-7B\"\n",
    "\n",
    "# --- 1. Load Models and Tokenizers ---\n",
    "print(f\"Loading base model: {BASE_MODEL_NAME}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, device_map=\"auto\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "print(f\"Loading nudging model: {NUDGING_MODEL_NAME}\")\n",
    "nudging_model = AutoModelForCausalLM.from_pretrained(NUDGING_MODEL_NAME, device_map=\"auto\")\n",
    "nudging_tokenizer = AutoTokenizer.from_pretrained(NUDGING_MODEL_NAME) # Assuming compatible\n",
    "\n",
    "# Set pad_token_id if not present\n",
    "for tokenizer, model in [(base_tokenizer, base_model), (nudging_tokenizer, nudging_model)]:\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    # Ensure use_cache is enabled in model config (usually true by default for generation models)\n",
    "    model.config.use_cache = True\n",
    "\n",
    "base_model.eval()\n",
    "nudging_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78421a54",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15cb833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUDGING PROMPT:  [INST] <<SYS>>\n",
      "Answer the question by walking through the reasoning steps.\n",
      "<</SYS>>\n",
      "\n",
      "Question: Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos? [/INST] \n",
      "BASE PROMPT:  Answer the question by walking through the reasoning steps.\n",
      "Question: Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import apply_instruct_template\n",
    "from dataset_utils import SYSTEM_PROMPT_REASONING\n",
    "QUESTION = \"Question: Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\"\n",
    "PROMPT_NUDGING = apply_instruct_template(model_name=NUDGING_MODEL_NAME, system_prompt=SYSTEM_PROMPT_REASONING, instruct_prompt=QUESTION, response_prompt=\"\", add_bos=False)\n",
    "PROMPT_BASE = apply_instruct_template(model_name=BASE_MODEL_NAME, system_prompt=SYSTEM_PROMPT_REASONING, instruct_prompt=QUESTION, response_prompt=\"\", add_bos=False)\n",
    "print(\"NUDGING PROMPT: \", PROMPT_NUDGING)\n",
    "print(\"BASE PROMPT: \", PROMPT_BASE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0507db",
   "metadata": {},
   "source": [
    "## Nudging with Caching implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa08ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warming up GPU...\n",
      "\n",
      "--- Running Baseline Generation with KV Cache ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (Baseline w/ Cache): Warmup: 100\n",
      "Time taken (Baseline w/ Cache): 2.0211 seconds\n",
      "Tokens generated (Baseline w/ Cache): 5\n",
      "Tokens per second (Baseline w/ Cache): 2.47\n",
      "\n",
      "--- Running Baseline Generation with KV Cache ---\n",
      "Generated text (Baseline w/ Cache): Warmup: 100\n",
      "Time taken (Baseline w/ Cache): 2.0205 seconds\n",
      "Tokens generated (Baseline w/ Cache): 5\n",
      "Tokens per second (Baseline w/ Cache): 2.47\n",
      "Warmup done.\n",
      "\n",
      "--- Running Baseline Generation with KV Cache ---\n",
      "Generated text (Baseline w/ Cache): Answer the question by walking through the reasoning steps.\n",
      "Question: Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\n",
      "Answer: 18 more pink flamingos than white flamingos.\n",
      "Reasoning: On Friday, there were 18 pink flamingos. On Saturday, there were 12 pink flamingos and 6 white flamingos. On Sunday, there were 18 pink flamingos and 6 white flamingos. So, there were 18 more pink flamingos than white flamingos. The 2018-2019 school year is off to a great start! We are excited to welcome our new students and families to the school. We are also excited to welcome back our returning students and families. We are looking forward to a great year!\n",
      "We are excited to announce that we have a new principal, Mr. Michael Hicks. Mr. Hicks has been a teacher and administrator in the district for many years. He is a great addition to our school.\n",
      "We are also excited to announce that we have a new assistant principal, Mrs. Jennifer Smith. Mrs. Smith has been a teacher in the district for many years. She is a great addition to our school.\n",
      "We are looking forward to a great year! We are excited to see what the future holds for our school.\n",
      "1 What is the best school in the world?\n",
      "2 What is the number 1 school in the US?\n",
      "3 What is the number 1 school in the world?\n",
      "4 What is the number 1 school in the world 2022?\n",
      "5 What is the number 1 school in the world 2022?\n",
      "6 What is the number 1 school in the world 2022?\n",
      "7 What is the number 1 school in the world 2022?\n",
      "What is the best school in the world?\n",
      "There is no one-size-fits-all answer to this question, as the best school for one student may not be the best school for another. However, there are a few factors that can help you determine which school is the best fit for your child.\n",
      "One important factor to consider is the school’s academic program. Does the school offer a rigorous curriculum that will challenge your child? Does the school have a good reputation for academic excellence?\n",
      "Another important factor to consider is the school’s extracurr\n",
      "Time taken (Baseline w/ Cache): 205.9002 seconds\n",
      "Tokens generated (Baseline w/ Cache): 500\n",
      "Tokens per second (Baseline w/ Cache): 2.43\n",
      "\n",
      "--- Running Nudging Generation with KV Cache ---\n",
      "Generated text (Nudging w/ Cache): Answer the question by walking through the reasoning steps.\n",
      "Question: Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\n",
      " To solve this problem, we need to use the information provided in the question to determine how many pink and white flamingos are out on Sue's front yard at noon on Sunday.\n",
      "\n",
      "Step 1: Identify the initial number of pink flamingos\n",
      "According to the question, on Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard. So, the initial number of pink flamingos is 18.\n",
      "\n",
      "Step 2: Identify the number of white flamingos\n",
      "According to the question, on Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard. Since they took back one third of the flamingos, the number of white flamingos is 18/3 = 6.\n",
      "\n",
      "Step 3: Identify the number of pink flamingos after Sunday morning\n",
      "According to the question, on Sunday morning, they added another 18 pink plastic flamingos to the collection. So, the number of pink flamingos after Sunday morning is 18 + 18 = 36.\n",
      "\n",
      "Step 4: Identify the number of white flamingos after Sunday morning\n",
      "According to the question, on Sunday morning, they added another 18 pink plastic flamingos to the collection. So, the number of white flamingos after Sunday morning is 6.\n",
      "\n",
      "Step 5: Find the difference between the number of pink flamingos and the number of white flamingos\n",
      "The difference between the number of pink flamingos and the number of white flamingos is 36 - 6 = 30.\n",
      "\n",
      "Step 6: Find the answer\n",
      "At noon on Sunday, there are 30 more pink plastic flamingos out than white plastic flamingos.\n",
      "\n",
      "Therefore, the answer is 30.\n",
      "Time taken (Nudging w/ Cache): 202.9804 seconds\n",
      "Tokens generated (Nudging w/ Cache): 440\n",
      "Tokens per second (Nudging w/ Cache): 2.17\n",
      "\n",
      "--- Comparison Summary (with KV Caching) ---\n",
      "Prompt: \"Question: Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\"\n",
      "Max new tokens: 500, Num nudging tokens (K): 16\n",
      "\n",
      "Baseline (Target Model Only w/ Cache):\n",
      "  Output: Answer the question by walking through the reasoning steps.\n",
      "Question: Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\n",
      "Answer: 18 more pink flamingos than white flamingos.\n",
      "Reasoning: On Friday, there were 18 pink flamingos. On Saturday, there were 12 pink flamingos and 6 white flamingos. On Sunday, there were 18 pink flamingos and 6 white flamingos. So, there were 18 more pink flamingos than white flamingos. The 2018-2019 school year is off to a great start! We are excited to welcome our new students and families to the school. We are also excited to welcome back our returning students and families. We are looking forward to a great year!\n",
      "We are excited to announce that we have a new principal, Mr. Michael Hicks. Mr. Hicks has been a teacher and administrator in the district for many years. He is a great addition to our school.\n",
      "We are also excited to announce that we have a new assistant principal, Mrs. Jennifer Smith. Mrs. Smith has been a teacher in the district for many years. She is a great addition to our school.\n",
      "We are looking forward to a great year! We are excited to see what the future holds for our school.\n",
      "1 What is the best school in the world?\n",
      "2 What is the number 1 school in the US?\n",
      "3 What is the number 1 school in the world?\n",
      "4 What is the number 1 school in the world 2022?\n",
      "5 What is the number 1 school in the world 2022?\n",
      "6 What is the number 1 school in the world 2022?\n",
      "7 What is the number 1 school in the world 2022?\n",
      "What is the best school in the world?\n",
      "There is no one-size-fits-all answer to this question, as the best school for one student may not be the best school for another. However, there are a few factors that can help you determine which school is the best fit for your child.\n",
      "One important factor to consider is the school’s academic program. Does the school offer a rigorous curriculum that will challenge your child? Does the school have a good reputation for academic excellence?\n",
      "Another important factor to consider is the school’s extracurr\n",
      "  Time: 205.9002s, Tokens: 500, TPS: 2.43\n",
      "\n",
      "Nudging (Base + Nudging w/ Cache):\n",
      "  Output: Answer the question by walking through the reasoning steps.\n",
      "Question: Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue.  On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard.  On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\n",
      " To solve this problem, we need to use the information provided in the question to determine how many pink and white flamingos are out on Sue's front yard at noon on Sunday.\n",
      "\n",
      "Step 1: Identify the initial number of pink flamingos\n",
      "According to the question, on Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard. So, the initial number of pink flamingos is 18.\n",
      "\n",
      "Step 2: Identify the number of white flamingos\n",
      "According to the question, on Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard. Since they took back one third of the flamingos, the number of white flamingos is 18/3 = 6.\n",
      "\n",
      "Step 3: Identify the number of pink flamingos after Sunday morning\n",
      "According to the question, on Sunday morning, they added another 18 pink plastic flamingos to the collection. So, the number of pink flamingos after Sunday morning is 18 + 18 = 36.\n",
      "\n",
      "Step 4: Identify the number of white flamingos after Sunday morning\n",
      "According to the question, on Sunday morning, they added another 18 pink plastic flamingos to the collection. So, the number of white flamingos after Sunday morning is 6.\n",
      "\n",
      "Step 5: Find the difference between the number of pink flamingos and the number of white flamingos\n",
      "The difference between the number of pink flamingos and the number of white flamingos is 36 - 6 = 30.\n",
      "\n",
      "Step 6: Find the answer\n",
      "At noon on Sunday, there are 30 more pink plastic flamingos out than white plastic flamingos.\n",
      "\n",
      "Therefore, the answer is 30.\n",
      "  Time: 202.9804s, Tokens: 440, TPS: 2.17\n",
      "\n",
      "Speedup (Nudging w/ Cache vs. Baseline w/ Cache): 1.01x\n"
     ]
    }
   ],
   "source": [
    "# Helper function to slice past_key_values to a new target sequence length\n",
    "def slice_past_key_values(past_key_values, new_seq_length):\n",
    "    if past_key_values is None:\n",
    "        return None\n",
    "    sliced_pkv = []\n",
    "    for layer_past in past_key_values:\n",
    "        # Each layer_past is a tuple of (key_states, value_states)\n",
    "        # key_states and value_states shape: [batch_size, num_heads, current_sequence_length, head_dim]\n",
    "        sliced_key = layer_past[0][:, :, :new_seq_length, :]\n",
    "        sliced_value = layer_past[1][:, :, :new_seq_length, :]\n",
    "        if sliced_key.shape[2] == 0: # If new_seq_length is 0, effectively reset\n",
    "            return None\n",
    "        sliced_pkv.append((sliced_key, sliced_value))\n",
    "    return tuple(sliced_pkv)\n",
    "\n",
    "# --- Baseline: Large (Target) Model Alone with KV Caching ---\n",
    "def generate_baseline_with_cache(model, tokenizer, prompt, max_new_tokens):\n",
    "    print(\"\\n--- Running Baseline Generation with KV Cache ---\")\n",
    "    input_ids_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    generated_ids = input_ids_prompt.clone()\n",
    "    \n",
    "    current_input_ids = input_ids_prompt\n",
    "    past_key_values = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_new_tokens):\n",
    "            outputs = model(current_input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "            next_token_logits = outputs.logits[:, -1, :] # Logits for the last token in the input\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "            past_key_values = outputs.past_key_values\n",
    "            current_input_ids = next_token # Next input is just the new token\n",
    "            \n",
    "            # if next_token.item() == tokenizer.eos_token_id:\n",
    "            #     print(f\"EOS token generated at step {i+1}.\")\n",
    "            #     break\n",
    "            if generated_ids.shape[1] >= tokenizer.model_max_length:\n",
    "                print(\"Max model length reached.\")\n",
    "                break\n",
    "                \n",
    "    if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    generation_time = end_time - start_time\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    num_actually_generated = generated_ids.shape[1] - input_ids_prompt.shape[1]\n",
    "    \n",
    "    print(f\"Generated text (Baseline w/ Cache): {generated_text}\")\n",
    "    print(f\"Time taken (Baseline w/ Cache): {generation_time:.4f} seconds\")\n",
    "    print(f\"Tokens generated (Baseline w/ Cache): {num_actually_generated}\")\n",
    "    if generation_time > 0:\n",
    "        print(f\"Tokens per second (Baseline w/ Cache): {num_actually_generated / generation_time:.2f}\")\n",
    "    return generated_text, generation_time, num_actually_generated\n",
    "\n",
    "# --- Nudging with KV Caching ---\n",
    "def generate_nudging_with_cache(base_model, \n",
    "                                nudging_model, \n",
    "                                base_tokenizer, \n",
    "                                nudging_tokenizer, \n",
    "                                base_prompt, \n",
    "                                nudging_prompt,\n",
    "                                max_new_tokens, \n",
    "                                num_nudging_tokens,\n",
    "                                threshold=0.4,\n",
    "                                base_spec_size=4,\n",
    "                                debug=False):\n",
    "    print(\"\\n--- Running Nudging Generation with KV Cache ---\")\n",
    "    device_base_model = next(base_model.parameters()).device\n",
    "    device_nudging_model = next(nudging_model.parameters()).device\n",
    "    input_ids_base_prompt = base_tokenizer.encode(base_prompt, return_tensors=\"pt\").to(device_base_model)\n",
    "    input_ids_nudging_prompt = nudging_tokenizer.encode(nudging_prompt, return_tensors=\"pt\").to(device_nudging_model)\n",
    "    prompt_len = input_ids_base_prompt.shape[1]\n",
    "    \n",
    "    generated_ids = input_ids_base_prompt.clone()\n",
    "    generated_ids_for_nudging = input_ids_nudging_prompt.clone()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
    "    \n",
    "    base_outputs = base_model(generated_ids[:, :-1], past_key_values=None, use_cache=True)\n",
    "    base_leave_last_one_past_kv = base_outputs.past_key_values\n",
    "    \n",
    "    nudging_outputs = nudging_model(generated_ids_for_nudging[:, :-1], past_key_values=None, use_cache=True)\n",
    "    nudging_leave_last_one_past_kv = nudging_outputs.past_key_values\n",
    "    \n",
    "    current_total_generated_after_prompt = 0\n",
    "    highlighed_full_output = \"\" # for debugging\n",
    "    \n",
    "    if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while current_total_generated_after_prompt < max_new_tokens:\n",
    "            len_at_step_start = generated_ids.shape[1]\n",
    "            len_at_step_start_nudging = generated_ids_for_nudging.shape[1]\n",
    "            \n",
    "            # Step 1: Use the nudging model to generate num_nudging_tokens tokens\n",
    "            nudging_suffix_ids_list = []\n",
    "            temp_nudging_gen_ids = generated_ids_for_nudging.clone()\n",
    "            current_nudging_model_input = generated_ids_for_nudging[:, -1].unsqueeze(1)\n",
    "            nudging_eos = False\n",
    "            \n",
    "            for i in range(num_nudging_tokens):\n",
    "                if temp_nudging_gen_ids.shape[1] >= tokenizer.model_max_length - 1: nudging_eos = True; break\n",
    "                if current_total_generated_after_prompt + len(nudging_suffix_ids_list) >= max_new_tokens: nudging_eos = True; break\n",
    "                \n",
    "                nudging_outputs = nudging_model(\n",
    "                    current_nudging_model_input,\n",
    "                    past_key_values=nudging_leave_last_one_past_kv,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                \n",
    "                next_nudging_token_logits = nudging_outputs.logits[:, -1, :]\n",
    "                next_nudging_token = torch.argmax(next_nudging_token_logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                nudging_suffix_ids_list.append(next_nudging_token)\n",
    "                temp_nudging_gen_ids = torch.cat([temp_nudging_gen_ids, next_nudging_token], dim=1)\n",
    "                \n",
    "                nudging_leave_last_one_past_kv = nudging_outputs.past_key_values\n",
    "                current_nudging_model_input = next_nudging_token\n",
    "                \n",
    "                if next_nudging_token.item() == tokenizer.eos_token_id:\n",
    "                    nudging_eos = True  \n",
    "                    break\n",
    "            if debug:\n",
    "                # print all nudging tokens\n",
    "                print(\"All nudging tokens:\")\n",
    "                all_nudging_tokens = [nudging_tokenizer.convert_ids_to_tokens(token[0].item()) for token in nudging_suffix_ids_list]\n",
    "                all_nudging_tokens = [token.replace(\"Ġ\", \" \") for token in all_nudging_tokens]\n",
    "                print(all_nudging_tokens)\n",
    "                \n",
    "            if nudging_eos: # if nudging model generates eos, we append all the nudging tokens to the generated ids\n",
    "                nudging_suffix_ids = torch.cat(nudging_suffix_ids_list, dim=1).to(device_base_model)\n",
    "                generated_ids = torch.cat([generated_ids, nudging_suffix_ids], dim=1)   # !!! potential bug here, nudging and base model might use different tokenizers\n",
    "                current_total_generated_after_prompt += num_nudging_tokens\n",
    "                break\n",
    "            \n",
    "            # Step 2: Use base model to check how many nudging tokens are accepted\n",
    "            nudging_suffix_ids_list = nudging_suffix_ids_list[:base_spec_size]\n",
    "            nudging_suffix_ids = torch.cat(nudging_suffix_ids_list, dim=1).to(device_base_model)    # !!! potential bug here, nudging and base model might use different tokenizers\n",
    "            base_input = torch.cat([generated_ids[:, -1].unsqueeze(1), nudging_suffix_ids], dim=1)\n",
    "            base_outputs = base_model(base_input, past_key_values=base_leave_last_one_past_kv, use_cache=True)\n",
    "            base_leave_last_one_past_kv = base_outputs.past_key_values  # to be sliced later\n",
    "            \n",
    "            # find the base model's top-1 token for all nudging suffix ids\n",
    "            base_logits = base_outputs.logits[:, :, :]\n",
    "            base_probs = torch.softmax(base_logits, dim=-1)\n",
    "            top_1_token_probs, _ = torch.max(base_probs, dim=-1, keepdim=True)\n",
    "            assert top_1_token_probs.shape == (1, base_spec_size + 1, 1) # top-1 token prob for all nudging suffix ids + the next token\n",
    "            \n",
    "            # Find the first token that the base model can take over\n",
    "            base_takeover_idx = base_spec_size\n",
    "            i = 1\n",
    "            while i < base_spec_size:\n",
    "                next_word_id = base_spec_size\n",
    "                # Find the first nudging token after i that contains a space (next word)\n",
    "                for j in range(i, base_spec_size):\n",
    "                    token = nudging_tokenizer.convert_ids_to_tokens(nudging_suffix_ids_list[j][0].item())\n",
    "                    if token.startswith(\"▁\") or token.startswith(\"Ġ\"):\n",
    "                        next_word_id = j\n",
    "                        break\n",
    "                if next_word_id == base_spec_size:  # if no next word (all tokens are a single word), accept all the nudging tokens\n",
    "                    base_takeover_idx = base_spec_size\n",
    "                    break\n",
    "                else:\n",
    "                    if top_1_token_probs[0, next_word_id, 0].item() > threshold:\n",
    "                        base_takeover_idx = next_word_id\n",
    "                        break\n",
    "                    else:\n",
    "                        i = next_word_id + 1\n",
    "            \n",
    "            if debug:\n",
    "                # print accepted nudging tokens\n",
    "                print(\"Accepted nudging tokens:\")\n",
    "                accepted_nudging_tokens = [nudging_tokenizer.convert_ids_to_tokens(nudging_suffix_ids_list[i][0].item()) for i in range(base_takeover_idx)]\n",
    "                accepted_nudging_tokens = [token.replace(\"Ġ\", \" \") for token in accepted_nudging_tokens]\n",
    "                nudging_text = \"\".join(accepted_nudging_tokens)\n",
    "                highlighed_full_output += f\"\\\\textbf{{{nudging_text}}}\"\n",
    "                print(accepted_nudging_tokens)\n",
    "                \n",
    "            if base_takeover_idx == base_spec_size: # accept all the nudging tokens\n",
    "                accepted_sequence_len_this_step = base_spec_size\n",
    "                # update the base model's past key values\n",
    "                base_leave_last_one_past_kv = slice_past_key_values(\n",
    "                    base_leave_last_one_past_kv,\n",
    "                    len_at_step_start - 1 + accepted_sequence_len_this_step\n",
    "                )\n",
    "                # update the base model's generated ids\n",
    "                generated_ids = torch.cat([generated_ids, nudging_suffix_ids], dim=1)\n",
    "                current_total_generated_after_prompt += accepted_sequence_len_this_step\n",
    "                # no need to update the nudging model's past key values\n",
    "                # update the generated ids for the nudging model\n",
    "                generated_ids_for_nudging = torch.cat([generated_ids_for_nudging, nudging_suffix_ids.to(device_nudging_model)], dim=1)\n",
    "            else:\n",
    "                accepted_sequence_len_this_step = base_takeover_idx + 1 # accept the first base_takeover_idx nudging tokens + the base model's next token\n",
    "                base_next_token = torch.argmax(base_logits[:, base_takeover_idx, :], dim=-1, keepdim=True)\n",
    "                # update the base model's past key values\n",
    "                base_leave_last_one_past_kv = slice_past_key_values(\n",
    "                    base_leave_last_one_past_kv,\n",
    "                    len_at_step_start - 1 + accepted_sequence_len_this_step\n",
    "                )\n",
    "                # update the base model's generated ids\n",
    "                generated_ids = torch.cat([generated_ids, nudging_suffix_ids[:, :base_takeover_idx], base_next_token], dim=1)\n",
    "                current_total_generated_after_prompt += accepted_sequence_len_this_step\n",
    "                # update the generated ids for the nudging model\n",
    "                generated_ids_for_nudging = torch.cat([generated_ids_for_nudging, nudging_suffix_ids[:, :base_takeover_idx].to(device_nudging_model), base_next_token.to(device_nudging_model)], dim=1)\n",
    "                # update the nudging model's past key values\n",
    "                nudging_leave_last_one_past_kv = slice_past_key_values(\n",
    "                    nudging_leave_last_one_past_kv,\n",
    "                    len_at_step_start_nudging - 1 + accepted_sequence_len_this_step\n",
    "                )\n",
    "            \n",
    "            # Step 3: Use the base model to generate tokens until \n",
    "            # 1. EOS token is generated\n",
    "            # 2. The number of generated tokens reaches max_new_tokens\n",
    "            # 3. The base model's top-1 token probability is below the threshold\n",
    "            base_tokens = [base_next_token] if base_takeover_idx != base_spec_size else []\n",
    "            while current_total_generated_after_prompt < max_new_tokens:\n",
    "                if generated_ids[0, -1].item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "                base_outputs = base_model(generated_ids[:, -1].unsqueeze(1), past_key_values=base_leave_last_one_past_kv, use_cache=True)\n",
    "                base_logits = base_outputs.logits[:, -1, :]\n",
    "                base_probs = torch.softmax(base_logits, dim=-1)\n",
    "                top_1_token_prob, next_base_token = torch.max(base_probs, dim=-1, keepdim=True)\n",
    "                if top_1_token_prob > threshold:\n",
    "                    base_tokens.append(next_base_token)\n",
    "                    generated_ids = torch.cat([generated_ids, next_base_token], dim=1)\n",
    "                    current_total_generated_after_prompt += 1\n",
    "                    base_leave_last_one_past_kv = base_outputs.past_key_values\n",
    "                    # update the nudging model's past key values and generated ids\n",
    "                    nudging_outputs = nudging_model(generated_ids_for_nudging[:, -1].unsqueeze(1), past_key_values=nudging_leave_last_one_past_kv, use_cache=True)\n",
    "                    generated_ids_for_nudging = torch.cat([generated_ids_for_nudging, next_base_token.to(device_nudging_model)], dim=1)\n",
    "                    nudging_leave_last_one_past_kv = nudging_outputs.past_key_values\n",
    "                else:\n",
    "                    break\n",
    "            if debug:\n",
    "                # print accepted base tokens\n",
    "                print(\"Accepted base tokens:\")\n",
    "                accepted_base_tokens = [base_tokenizer.convert_ids_to_tokens(token[0].item()) for token in base_tokens]\n",
    "                accepted_base_tokens = [token.replace(\"Ġ\", \" \") for token in accepted_base_tokens]\n",
    "                base_text = \"\".join(accepted_base_tokens)\n",
    "                highlighed_full_output += base_text\n",
    "                print(accepted_base_tokens)\n",
    "                print(\"After a round of generation:\")\n",
    "                print(highlighed_full_output)\n",
    "                print(\"************************************************\")\n",
    "                # print all decoded generated ids\n",
    "                print([base_tokenizer.convert_ids_to_tokens(generated_ids[0, i].item()) for i in range(generated_ids.shape[1])])\n",
    "                # print all decoded generated ids for nudging model\n",
    "                print([nudging_tokenizer.convert_ids_to_tokens(generated_ids_for_nudging[0, i].item()) for i in range(generated_ids_for_nudging.shape[1])])\n",
    "                print(\"--------------------------------\")\n",
    "            \n",
    "            if current_total_generated_after_prompt >= max_new_tokens: break\n",
    "            if generated_ids[0, -1].item() == tokenizer.eos_token_id and generated_ids.shape[1] > prompt_len : break\n",
    "\n",
    "    if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    final_generated_sequence = generated_ids[0, :prompt_len + current_total_generated_after_prompt]\n",
    "    generated_text = base_tokenizer.decode(final_generated_sequence, skip_special_tokens=True)\n",
    "    num_actually_generated = final_generated_sequence.shape[0] - prompt_len\n",
    "    \n",
    "    print(f\"Generated text (Nudging w/ Cache): {generated_text}\")\n",
    "    print(f\"Time taken (Nudging w/ Cache): {generation_time:.4f} seconds\")\n",
    "    print(f\"Tokens generated (Nudging w/ Cache): {num_actually_generated}\")\n",
    "    if generation_time > 0:\n",
    "        print(f\"Tokens per second (Nudging w/ Cache): {num_actually_generated / generation_time:.2f}\")\n",
    "        \n",
    "    return generated_text, generation_time, num_actually_generated\n",
    "\n",
    "\n",
    "# --- Run Comparison ---\n",
    "if __name__ == \"__main__\":\n",
    "    MAX_NEW_TOKENS = 500\n",
    "    NUM_NUDGING_TOKENS = 16\n",
    "    target_model = base_model\n",
    "    draft_model = nudging_model\n",
    "    target_tokenizer = base_tokenizer\n",
    "    nudging_tokenizer = nudging_tokenizer\n",
    "    \n",
    "    # Warm-up GPU\n",
    "    if DEVICE == \"cuda\":\n",
    "        print(\"\\nWarming up GPU...\")\n",
    "        for _ in range(2):\n",
    "            _ = generate_baseline_with_cache(target_model, target_tokenizer, \"Warmup\", 5)\n",
    "        print(\"Warmup done.\")\n",
    "\n",
    "    # Baseline with Cache\n",
    "    text_base_cache, time_base_cache, tokens_base_cache = generate_baseline_with_cache(\n",
    "        target_model, target_tokenizer, PROMPT_BASE, MAX_NEW_TOKENS\n",
    "    )\n",
    "\n",
    "    # Nudging with Cache\n",
    "    text_nudging_cache, time_nudging_cache, tokens_nudging_cache = generate_nudging_with_cache(\n",
    "        base_model, \n",
    "        nudging_model, \n",
    "        base_tokenizer, \n",
    "        nudging_tokenizer, \n",
    "        PROMPT_BASE, \n",
    "        PROMPT_NUDGING, \n",
    "        MAX_NEW_TOKENS, \n",
    "        NUM_NUDGING_TOKENS, \n",
    "        threshold=0.4,\n",
    "        base_spec_size=NUM_NUDGING_TOKENS,\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Comparison Summary (with KV Caching) ---\")\n",
    "    print(f\"Prompt: \\\"{QUESTION}\\\"\")\n",
    "    print(f\"Max new tokens: {MAX_NEW_TOKENS}, Num nudging tokens (K): {NUM_NUDGING_TOKENS}\")\n",
    "    \n",
    "    print(\"\\nBaseline (Target Model Only w/ Cache):\")\n",
    "    print(f\"  Output: {text_base_cache}\")\n",
    "    print(f\"  Time: {time_base_cache:.4f}s, Tokens: {tokens_base_cache}, TPS: {tokens_base_cache / (time_base_cache + 1e-9):.2f}\")\n",
    "\n",
    "    print(\"\\nNudging (Base + Nudging w/ Cache):\")\n",
    "    print(f\"  Output: {text_nudging_cache}\")\n",
    "    print(f\"  Time: {time_nudging_cache:.4f}s, Tokens: {tokens_nudging_cache}, TPS: {tokens_nudging_cache / (time_nudging_cache + 1e-9):.2f}\")\n",
    "\n",
    "    if time_base_cache > 0 and time_nudging_cache > 0:\n",
    "        speedup = time_base_cache / time_nudging_cache\n",
    "        print(f\"\\nSpeedup (Nudging w/ Cache vs. Baseline w/ Cache): {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25503f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explaining_icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
